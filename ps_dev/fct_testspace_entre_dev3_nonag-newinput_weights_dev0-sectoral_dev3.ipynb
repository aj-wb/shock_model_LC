{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import glob,os\n",
    "import sys\n",
    "import scipy\n",
    "from importlib import  reload\n",
    "from time import process_time \n",
    "#from libraries.lib_gather_data import get_hhid_FIES\n",
    "from datetime import datetime\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#from shock_libraries import *\n",
    "#from plotting_libraries import *\n",
    "#from response_libraries import get_response_sp\n",
    "#\n",
    "from income_shock_libraries_ps import *\n",
    "#\n",
    "#from libraries.lib_country_dir import set_directories, load_survey_data, get_places_dict\n",
    "#from libraries.lib_get_hh_savings import get_hh_savings\n",
    "#from libraries.pandas_helper import broadcast_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting & aesthetics\n",
    "font = {'family':'sans serif', 'size':10}\n",
    "plt.rc('font', **font)\n",
    "mpl.rcParams['xtick.labelsize'] = 10\n",
    "mpl.rcParams['ytick.labelsize'] = 10\n",
    "mpl.rcParams['legend.facecolor'] = 'white'\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "sns_pal = sns.color_palette('Set1', n_colors=8, desat=.4)\n",
    "greys_pal = sns.color_palette('Greys', n_colors=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lol = pd.read_csv('./2015FIES/LFSJul2015_merge.csv')\n",
    "# lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def entre_shock(s_sector='LFS_sector',selector_type = 0):\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "s_sector = 'LFS_sector' #### <HARDCODE> testing\n",
    "\n",
    "#mr = merge_rank()\n",
    "#mr = merge_rank(rank_file='./temp/lfs_a09_pqkb_ranked_V2_entrpreneurial_20200423.csv',labor_file='./csv/ph_labor_force.csv',outfile='./temp/_labor_rank_merge_test.csv',merge_col='a09_pqkb')\n",
    "mr = merge_rank(rank_file='./temp/lfs_a09_pqkb_ranked_V2_entrpreneurial_20200423.csv',labor_file='./csv/ph_labor_force.csv',outfile='./temp/_labor_rank_merge_test.csv',merge_col='a09_pqkb')\n",
    "\n",
    "\n",
    "\n",
    "if not 'LFS_sector' in mr.columns:\n",
    "    mr = mr.rename(columns={'LFS_sector_x': 'LFS_sector'})\n",
    "    # get subset: a09_pqkb\n",
    "\n",
    "if 'hhid_lfs' in mr.columns:\n",
    "    mr_subset = mr[['hhid_lfs','cc101_lno','LFS_sector','a09_pqkb','c19_pclass','demand_scale', 'w_home','E_sector','pwgt']]\n",
    "else:\n",
    "    mr_subset = mr[['cc101_lno','LFS_sector','a09_pqkb','c19_pclass','demand_scale', 'w_home','E_sector','pwgt']]\n",
    "    \n",
    "    \n",
    "mr_subset\n",
    "indexNames = mr_subset[mr_subset['a09_pqkb'] == 'nan' ].index\n",
    "# Delete these row indexes from dataFrame\n",
    "mr_subset.drop(indexNames , inplace=True)\n",
    "mr_subset = mr_subset.reset_index(drop=True)\n",
    "# get subset: c19_pclass\n",
    "\n",
    "indexNames2 = mr_subset[mr_subset['c19_pclass'] == 'nan' ].index\n",
    "\n",
    "# Delete these row indexes from dataFrame\n",
    "mr_subset.drop(indexNames2 , inplace=True)\n",
    "mr_subset = mr_subset.reset_index(drop=True)\n",
    "\n",
    "# make new column of combined string a09 && c19:\n",
    "mr_subset['a09c19'] = mr_subset['a09_pqkb'] +'-'+mr_subset['c19_pclass']\n",
    "\n",
    "# enforce string:\n",
    "mr_subset['a09_pqkb'] = [str(q).strip() for q in mr_subset['a09_pqkb']] # enforce type = string\n",
    "mr_subset['LFS_sector'] = [str(q).strip() for q in mr_subset['LFS_sector']] # enforce type = string\n",
    "mr_subset['c19_pclass'] = [str(q).strip() for q in mr_subset['c19_pclass']] # enforce type = string\n",
    "mr_subset['a09c19'] = [str(q).strip() for q in mr_subset['a09c19']] # enforce type = string\n",
    "if 'E_sector' in mr_subset.columns:\n",
    "    mr_subset['E_sector'] = [str(q).strip() for q in mr_subset['E_sector']] # enforce type = string\n",
    "\n",
    "\n",
    "## implement switch statement here:\n",
    "selector_type = 0 # 0 = default, 1 = nonag_wage, 2 = entrepreneurial ####### <HARDCODE> for testing\n",
    "\n",
    "if selector_type == 0:\n",
    "    df_select = mr_subset\n",
    "elif selector_type == 1:\n",
    "    df_select = mr_subset [~mr_subset['c19_pclass'].isin(['Self Employed', 'Employer','Without Pay (Family owned Business)'])] # NOT entrepreneurial income\n",
    "elif selector_type == 2:\n",
    "    df_select = mr_subset [mr_subset['c19_pclass'].isin(['Self Employed', 'Employer','Without Pay (Family owned Business)'])] # YES entrepreneurial income\n",
    "else:\n",
    "    print('df_selector error')\n",
    "\n",
    "# generate counts by subsector (a09..) and sectors (s_sector)\n",
    "df_select['sector_count'] = df_select.groupby(s_sector)['pwgt'].transform('sum') #count total unique sectors and append to mr_subset\n",
    "df_select['desc_count'] = df_select.groupby('a09_pqkb')['pwgt'].transform('sum') #count total unique sectors and append to mr_subset\n",
    "\n",
    "df_select['sector_frac'] = df_select['desc_count'] / df_select['sector_count'] # get fraction of sector as weightin\n",
    "\n",
    "# drop duplicates (now that overall weighting established)\n",
    "df_select = df_select.drop_duplicates(subset='a09_pqkb')\n",
    "df_select = df_select.reset_index(drop=True)\n",
    "\n",
    "# generate probability and combine with relative weighting\n",
    "df_select['partial_prob'] = np.nan\n",
    "df_select['third_col'] = np.nan\n",
    "df_select['dummy'] = np.nan\n",
    "\n",
    "    # incorporate Kayenat tables into 'di' &&\n",
    "    # nested logic to incorporate 0-4 scale for social distancing measures\n",
    "    ## where scores of 0 & 1 result in complete job lost, due to unable to distance\n",
    "i=0\n",
    "while i < len(df_select):\n",
    "\n",
    "    if df_select.demand_scale[i] == 0:\n",
    "\n",
    "        # incorporate 0-4 scale logic:\n",
    "\n",
    "        if df_select.w_home[i] == 0:\n",
    "            df_select.partial_prob[i] = 0\n",
    "\n",
    "        elif df_select.w_home[i] == 1:\n",
    "            df_select.partial_prob[i] = 0\n",
    "\n",
    "        else:\n",
    "            df_select.partial_prob[i] = df_select.sector_frac[i] * (random.randint(0,50)/100)\n",
    "\n",
    "\n",
    "    elif df_select.demand_scale[i] == 0.5: \n",
    "\n",
    "        # incorporate 0-4 scale logic:\n",
    "        if df_select.w_home[i] == 0:\n",
    "            df_select.partial_prob[i] = 0\n",
    "\n",
    "        elif df_select.w_home[i] == 1:\n",
    "            df_select.partial_prob[i] = 0\n",
    "\n",
    "        else: \n",
    "            df_select.partial_prob[i] = df_select.sector_frac[i] * (random.randint(50,100)/100)\n",
    "\n",
    "    elif df_select.demand_scale[i] == 1.0:\n",
    "        df_select.partial_prob[i] = df_select.sector_frac[i]\n",
    "    else:\n",
    "        df_select.dummy[i] = -99\n",
    "    \n",
    "\n",
    "# incorporate 3rd column modifiers here:\n",
    "    if (df_select['c19_pclass'][i] == \"Gov't/Gov't Corporation\"):\n",
    "        df_select.partial_prob[i] = 1  # essentially reverts the random uniform logic implemented above\n",
    "\n",
    "        \n",
    "    i = i + 1 \n",
    "    # remove nans in summing fields, and dummy storage\n",
    "del df_select['dummy']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# section--> get mean probability by sector:\n",
    "\n",
    "#storage['fa'] = np.nan\n",
    "storage = pd.DataFrame(columns=['fa', 'di'], index=[np.unique(df_select[s_sector])])\n",
    "storage.index.names = ['sector']\n",
    "\n",
    "# iterate through loop of sector names\n",
    "for seclist in np.unique(df_select[s_sector]):\n",
    "    pillow = 1 - (df_select[df_select[s_sector] == seclist].partial_prob.sum()) # KEY: logic inverted here-- now a probability of 1- change job loss = 'fraction affected'\n",
    "\n",
    "    storage.loc[seclist,'fa'] = pillow # pillow is just an intermediate storage variable\n",
    "    storage.loc[seclist,'di'] = 1\n",
    "    \n",
    "    print('seclist ='+ seclist)\n",
    "\n",
    "\n",
    "tstamp = (datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "storage.to_csv('./temp/table_full_'+s_sector+'_' +tstamp+'.csv')\n",
    "\n",
    "#    return(storage)\n",
    "storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select['partial_prob'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select[df_select[s_sector] == 'ag'].partial_prob.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_select.groupby('a09_pqkb')['pwgt'].transform('sum') #count total unique sectors and append to mr_subset\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generate counts by subsector (a09..) and sectors (s_sector)\n",
    "df_select['sector_count'] = df_select.groupby(s_sector)['pwgt'].transform('sum') #count total unique sectors and append to mr_subset\n",
    "df_select['desc_count'] = df_select.groupby('a09_pqkb')['pwgt'].transform('sum') #count total unique sectors and append to mr_subset\n",
    "\n",
    "df_select['sector_frac'] = df_select['desc_count'] / df_select['sector_count'] # get fraction of sector as weightin\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = entre_shock()\n",
    "xx\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shock_100_entre():  # initialize shock sector storage dataframe\n",
    "   \n",
    "    '''\n",
    "   current hard coding for sensitivity analysis, 20200413: requires cleaning for further implementation\n",
    "   - addition of modularity\n",
    "   - \n",
    "   - current functionality:\n",
    "       - outputs csv to location: './temp/sect_iter_100.csv\n",
    "       - containing data frame with 101 simulations of <rand_weighted_shock_distance():\n",
    "    - runtime: ~10minutes\n",
    "   '''\n",
    "\n",
    "    stor = entre_shock()\n",
    "    del stor['di']\n",
    "\n",
    "    # set number of iterations\n",
    "    p = 0\n",
    "    n_iter = 99\n",
    "\n",
    "    # model and store stochastic sector response\n",
    "    while p < n_iter:\n",
    "        \n",
    "        new_val = entre_shock()\n",
    "        del new_val['di']\n",
    "        new_val = new_val.rename(columns={'fa': ('iter'+str(p))})\n",
    "\n",
    "        # pd.merge(labor,rank, on=merge_col, how='left')\n",
    "        stor = pd.merge(stor,new_val,on='sector', how='left')\n",
    "        p = p+ 1\n",
    "        print(p)\n",
    "        \n",
    "    tstamp = (datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "    stor.to_csv('./temp/entre_shock_mc_fullsector_'+tstamp+ '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_shock_100_entre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shock_stats_entre():\n",
    "    # generate shock table statistics\n",
    "    #df['mean'] = df.mean(axis=1)\n",
    "\n",
    "    # load csv to dataframe:\n",
    "    #dfs = pd.read_csv('./temp/sect_iter_100.csv') # original\n",
    "    dfs = pd.read_csv('./temp/entre_shock_mc_fullsector_20200427_1252.csv') # modified 20200420\n",
    "    # set index to LFS_sector\n",
    "    dfs.set_index('sector')\n",
    "\n",
    "    # compute statistics:\n",
    "    dfs['mean'] = dfs.mean(axis=1)\n",
    "    #print(dfs['mean'])\n",
    "    dfs['std_dev'] = dfs.std(axis=1)\n",
    "    #print(dfs['std_dev'])\n",
    "\n",
    "    #round to 3 dec:\n",
    "    dfs['mean'] = [(round(q, 3)) for q in dfs['mean']]\n",
    "    dfs['std_dev'] = [(round(q, 3)) for q in dfs['std_dev']]\n",
    "\n",
    "    # new datafame storing just info:\n",
    "    df_stat = dfs[['sector','mean','std_dev']].set_index('sector')\n",
    "    df_stat\n",
    "    # df_stat.to_csv('./temp/phi_get_shock_input.csv') # original\n",
    "    tstamp = (datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "    df_stat.to_csv('./temp/phi_fullsector_'+tstamp+'.csv') # modified 20200420\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return(df_stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_shock_stats_entre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the shock table\n",
    "def get_phi_shock(flavor=0):\n",
    "    \"\"\"\n",
    "    20200426: loads shock tables\n",
    "    \n",
    "    input: flavor\n",
    "        options:\n",
    "            0 -- default shock table, in original model\n",
    "            1 -- sectoral shock table, based on scoring\n",
    "            2 -- nonag_shock table, sector names v2\n",
    "            3 -- entrepreneurial shock table, sector names v2\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    if flavor == 0:\n",
    "        # default shock table:\n",
    "        shock_default = { 'ag':           [  0,  0],\n",
    "                 'mining':        [  0,  0],\n",
    "                 'utilities':     [  0,  0],\n",
    "                 'construction':  [0.5,1.0],\n",
    "                 'manufacturing': [0.1,1.0],\n",
    "                 'wholesale':     [0.1,1.0],\n",
    "                 'retail':        [0.5,1.0],\n",
    "                 'transportation':[0.5,1.0],\n",
    "                 'information':   [0.1,1.0],\n",
    "                 'finance':       [0.1,1.0],\n",
    "                 'professional_services':[0.1,1.0],\n",
    "                 'eduhealth':     [0.1,1.0],\n",
    "                 'food_entertainment':[0.8,1.0],\n",
    "                 'government':    [  0,  0],\n",
    "                 'other':         [0.8,1.0]}\n",
    "        df_shock = pd.DataFrame(data=shock_default).T\n",
    "        df_shock.columns = ['fa','di']\n",
    "        df_shock.index.name = 'sector'\n",
    "        shock_table = df_shock\n",
    "    \n",
    "    if flavor == 1:\n",
    "        # sectoral shock table\n",
    "        df = pd.read_csv('./temp/phi_shocks/phi_shock_3dimv2.csv').set_index('LFS_sector')\n",
    "        df = df.rename(columns={'mean': 'fa', 'std_dev':'di'}) \n",
    "        df['di'] = 1\n",
    "        \n",
    "        #print(flavor)\n",
    "        #print(df)\n",
    "        shock_table = df\n",
    "        \n",
    "    if flavor == 2:\n",
    "        #nonag shock table\n",
    "        df = pd.read_csv('./temp/phi_shocks/phi_nonag.csv').set_index('sector')\n",
    "        df = df.rename(columns={'mean': 'fa', 'std_dev':'di'}) \n",
    "        df['di'] = 1\n",
    "       # print(flavor)\n",
    "        shock_table = df\n",
    "        \n",
    "    if flavor == 3:\n",
    "        #entrepreneurial shock table\n",
    "        df = pd.read_csv('./temp/phi_shocks/phi_entre.csv').set_index('sector')\n",
    "        df = df.rename(columns={'mean': 'fa', 'std_dev':'di'}) \n",
    "        df['di'] = 1\n",
    "        shock_table = df\n",
    "        \n",
    "        \n",
    "        #print(flavor)\n",
    "   \n",
    "    \n",
    "    return(shock_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = get_phi_shock(3)\n",
    "xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
