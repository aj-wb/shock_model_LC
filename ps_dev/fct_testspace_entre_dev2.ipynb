{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import glob,os\n",
    "import sys\n",
    "import scipy\n",
    "from importlib import  reload\n",
    "from time import process_time \n",
    "#from libraries.lib_gather_data import get_hhid_FIES\n",
    "from datetime import datetime\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from shock_libraries import *\n",
    "from plotting_libraries import *\n",
    "from response_libraries import get_response_sp\n",
    "#\n",
    "from income_shock_libraries_ps import *\n",
    "#\n",
    "from libraries.lib_country_dir import set_directories, load_survey_data, get_places_dict\n",
    "from libraries.lib_get_hh_savings import get_hh_savings\n",
    "from libraries.pandas_helper import broadcast_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting & aesthetics\n",
    "font = {'family':'sans serif', 'size':10}\n",
    "plt.rc('font', **font)\n",
    "mpl.rcParams['xtick.labelsize'] = 10\n",
    "mpl.rcParams['ytick.labelsize'] = 10\n",
    "mpl.rcParams['legend.facecolor'] = 'white'\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "sns_pal = sns.color_palette('Set1', n_colors=8, desat=.4)\n",
    "greys_pal = sns.color_palette('Greys', n_colors=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ORIGINAL FUNCTION\n",
    "#---------------------------------- Added: 20200422: <PS>\n",
    "\n",
    "def rand_weighted_shock_3dim_v2_edit():\n",
    "\n",
    "    \"\"\"    \n",
    "    Updated 20200422 \n",
    "        - incorporate 3rd dimension for sector (public/private/gov) and impact on essentiality\n",
    "            - now, only enforce that government jobs are maintained, across all sectors\n",
    "    \n",
    "    Updated 20200419 \n",
    "        - incorporate 3rd dimension for sector (public/private/gov) and impact on essentiality \n",
    "    Updated 20200413:\n",
    "        - incorporate 2nd dimension for social distancing potential\n",
    "    module added to covid_phl: <income_shock_libraries_ps.py>\n",
    "\n",
    "    primary development: <FCT> rand_weigthed_shock_distance() <FCT>\n",
    "\n",
    "    - function to replace: \n",
    "        rand_weighted_shock_3dim()--> rand_weighted_shock_distance() --> rand_weighted_shock_1() --> get_income_shock(): in <shock_libraries.py>\n",
    "\n",
    "    - description:\n",
    "        * matches existing df_shock dataframe (compatibiility)\n",
    "        * uses Kayenat table of job descriptions demand value for 'a09_pqkb' by sector to create weighted probability of job disruption by sector, as input to 'fa' column of df_shock             \n",
    "        dataframe -- representative FIES and LFS data \n",
    "\n",
    "       * for values 0.0, 0.5,1 : assigns each job description a random: 0-50%, 50-99%, 100% chance of disruption, weighting each by the prevalence of that role in each sector, to generate cumulative probability of disruption. \n",
    "\n",
    "\n",
    "        * now incorporates enforcement of social distancing measures, by enforcing social distance in non-essential jobs based on K.Kabirs' 0-4 'work-from-home' scoring.\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    # develop 3 factor code here:\n",
    "\n",
    "    # make each factor modular\n",
    "\n",
    "    mr = merge_rank()\n",
    "    if not 'LFS_sector' in mr.columns:\n",
    "        mr = mr.rename(columns={'LFS_sector_x': 'LFS_sector'})\n",
    "            # get subset: a09_pqkb\n",
    "    mr_subset = mr[['hhid_lfs','LFS_sector','cc101_lno','a09_pqkb','c19_pclass','demand_scale', 'w_home']]\n",
    "\n",
    "    indexNames = mr_subset[mr_subset['a09_pqkb'] == 'nan' ].index\n",
    "\n",
    "        # Delete these row indexes from dataFrame\n",
    "    mr_subset.drop(indexNames , inplace=True)\n",
    "    mr_subset = mr_subset.reset_index(drop=True)\n",
    "\n",
    "     # get subset: c19_pclass\n",
    "\n",
    "    indexNames2 = mr_subset[mr_subset['c19_pclass'] == 'nan' ].index\n",
    "\n",
    "        # Delete these row indexes from dataFrame\n",
    "    mr_subset.drop(indexNames2 , inplace=True)\n",
    "    mr_subset = mr_subset.reset_index(drop=True)\n",
    "\n",
    "    # make new column of combined string a09 && c19:\n",
    "    mr_subset['a09c19'] = mr_subset['a09_pqkb'] +'-'+mr_subset['c19_pclass']\n",
    "\n",
    "        # enforce string:\n",
    "    mr_subset['a09_pqkb'] = [str(q) for q in mr_subset['a09_pqkb']] # enforce type = string\n",
    "    mr_subset['LFS_sector'] = [str(q) for q in mr_subset['LFS_sector']] # enforce type = string\n",
    "    mr_subset['c19_pclass'] = [str(q) for q in mr_subset['c19_pclass']] # enforce type = string\n",
    "    mr_subset['a09c19'] = [str(q) for q in mr_subset['a09c19']] # enforce type = string\n",
    "\n",
    "\n",
    "        # generate fraction by sector\n",
    "    mr_subset['desc_count'] = mr_subset.groupby('a09_pqkb')['a09_pqkb'].transform('count')# count unique jobs and append to mr_subset\n",
    "    mr_subset['sector_count'] = mr_subset.groupby('LFS_sector')['LFS_sector'].transform('count') #count total unique sectors and append to mr_subset\n",
    "    mr_subset['sector_frac'] = mr_subset['desc_count'] / mr_subset['sector_count'] # get fraction of sector as weighting\n",
    "\n",
    "\n",
    "\n",
    "    #####\n",
    "    # here, need to insert a new column that merges a09 and c19 -- done\n",
    "    # then, drop duplicates off of this column, so that we can minimize computation\n",
    "\n",
    "    # still need logic to build the logic for each job sector\n",
    "    ## may need to restructure this whole section of code\n",
    "\n",
    "    #####\n",
    "        # drop duplicates (now that overall weighting established)\n",
    "    mr_subset = mr_subset.drop_duplicates(subset='a09_pqkb')\n",
    "    mr_subset = mr_subset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        # generate probability and combine with relative weighting\n",
    "    mr_subset['partial_prob'] = np.nan\n",
    "    mr_subset['third_col'] = np.nan\n",
    "    mr_subset['dummy'] = np.nan\n",
    "\n",
    "        # incorporate Kayenat tables into 'di' &&\n",
    "        # nested logic to incorporate 0-4 scale for social distancing measures\n",
    "        ## where scores of 0 & 1 result in complete job lost, due to unable to distance\n",
    "    i=0\n",
    "    while i < len(mr_subset):\n",
    "\n",
    "\n",
    "\n",
    "        if mr_subset.demand_scale[i] == 0:\n",
    "\n",
    "                # incorporate 0-4 scale logic:\n",
    "\n",
    "            if mr_subset.w_home[i] == 0:\n",
    "                mr_subset.partial_prob[i] = 1\n",
    "\n",
    "            elif mr_subset.w_home[i] == 1:\n",
    "                mr_subset.partial_prob[i] = 1\n",
    "\n",
    "            else:\n",
    "                mr_subset.partial_prob[i] = mr_subset.sector_frac[i] * (random.randint(0,50)/100)\n",
    "\n",
    "\n",
    "        elif mr_subset.demand_scale[i] == 0.5: \n",
    "\n",
    "                # incorporate 0-4 scale logic:\n",
    "            if mr_subset.w_home[i] == 0:\n",
    "                mr_subset.partial_prob[i] = 1\n",
    "\n",
    "            elif mr_subset.w_home[i] == 1:\n",
    "                mr_subset.partial_prob[i] = 1\n",
    "\n",
    "            else: \n",
    "                mr_subset.partial_prob[i] = mr_subset.sector_frac[i] * (random.randint(50,100)/100)\n",
    "\n",
    "        elif mr_subset.demand_scale[i] == 1.0:\n",
    "            mr_subset.partial_prob[i] = mr_subset.sector_frac[i]\n",
    "        else:\n",
    "            mr_subset.dummy[i] = -99\n",
    "\n",
    "            \n",
    "        # incorporate 3rd column modifiers here:\n",
    "        if (mr_subset['c19_pclass'][i] == \"Gov't/Gov't Corporation\"):\n",
    "            mr_subset.partial_prob[i] = 0  # essentially reverts the random uniform logic implemented above\n",
    "\n",
    "\n",
    "        i = i + 1\n",
    "\n",
    "        # remove nans in summing fields, and dummy storage\n",
    "    del mr_subset['dummy']\n",
    "\n",
    "        #define shock table:\n",
    "    shock_null = { 'ag':           [  0,  0],\n",
    "                     'mining':        [  0,  0],\n",
    "                     'utilities':     [  0,  0],\n",
    "                     'construction':  [0.0,1.0],\n",
    "                     'manufacturing': [0.0,1.0],\n",
    "                     'wholesale':     [0.0,1.0],\n",
    "                     'retail':        [0.0,1.0],\n",
    "                     'transportation':[0.0,1.0],\n",
    "                     'information':   [0.0,1.0],\n",
    "                     'finance':       [0.0,1.0],\n",
    "                     'professional_services':[0.0,1.0],\n",
    "                     'eduhealth':     [0.0,1.0],\n",
    "                     'food_entertainment':[0.0,1.0],\n",
    "                     'government':    [  0,  0],\n",
    "                     'other':         [0.0,1.0]}\n",
    "    df_shock_null = pd.DataFrame(data=shock_null).T\n",
    "    df_shock_null.columns = ['fa','di']\n",
    "    df_shock_null.index.name = 'LFS_sector'\n",
    "\n",
    "\n",
    "    df_shock_cum = df_shock_null\n",
    "\n",
    "        # get mean probability by sector:\n",
    "\n",
    "    for seclist in df_shock_cum.index: # hard-coded to existing shock table\n",
    "\n",
    "        pp = mr_subset[mr_subset.LFS_sector == seclist]\n",
    "        p4 = 1 - sum(pp.partial_prob)\n",
    "\n",
    "            # build shock table:\n",
    "        df_shock_cum['fa'][seclist] = df_shock_cum['fa'][seclist] + p4\n",
    "\n",
    "        # save to separate var for testing    \n",
    "    rand_weighted_shock = df_shock_cum\n",
    "    \n",
    "    return(rand_weighted_shock)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/geospatial/lib/python3.6/site-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n",
      "/opt/anaconda3/envs/geospatial/lib/python3.6/site-packages/ipykernel_launcher.py:114: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/envs/geospatial/lib/python3.6/site-packages/ipykernel_launcher.py:130: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/envs/geospatial/lib/python3.6/site-packages/ipykernel_launcher.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/envs/geospatial/lib/python3.6/site-packages/ipykernel_launcher.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/envs/geospatial/lib/python3.6/site-packages/ipykernel_launcher.py:124: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/envs/geospatial/lib/python3.6/site-packages/ipykernel_launcher.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/envs/geospatial/lib/python3.6/site-packages/ipykernel_launcher.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/anaconda3/envs/geospatial/lib/python3.6/site-packages/ipykernel_launcher.py:127: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fa</th>\n",
       "      <th>di</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LFS_sector</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ag</th>\n",
       "      <td>-3.954627e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mining</th>\n",
       "      <td>7.659851e-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>utilities</th>\n",
       "      <td>-8.034483e-01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>construction</th>\n",
       "      <td>9.235069e-01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manufacturing</th>\n",
       "      <td>-1.202379e+02</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wholesale</th>\n",
       "      <td>-1.532032e+01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retail</th>\n",
       "      <td>-3.766706e+01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transportation</th>\n",
       "      <td>-1.207113e+01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>information</th>\n",
       "      <td>1.110223e-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>-8.564095e-01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>professional_services</th>\n",
       "      <td>-3.626933e+00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eduhealth</th>\n",
       "      <td>-9.211073e+00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>food_entertainment</th>\n",
       "      <td>-1.802994e+01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>government</th>\n",
       "      <td>-1.008873e+00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>-1.801616e+01</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 fa   di\n",
       "LFS_sector                              \n",
       "ag                    -3.954627e+00  0.0\n",
       "mining                 7.659851e-01  0.0\n",
       "utilities             -8.034483e-01  0.0\n",
       "construction           9.235069e-01  1.0\n",
       "manufacturing         -1.202379e+02  1.0\n",
       "wholesale             -1.532032e+01  1.0\n",
       "retail                -3.766706e+01  1.0\n",
       "transportation        -1.207113e+01  1.0\n",
       "information            1.110223e-16  1.0\n",
       "finance               -8.564095e-01  1.0\n",
       "professional_services -3.626933e+00  1.0\n",
       "eduhealth             -9.211073e+00  1.0\n",
       "food_entertainment    -1.802994e+01  1.0\n",
       "government            -1.008873e+00  0.0\n",
       "other                 -1.801616e+01  1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = rand_weighted_shock_3dim_v2_edit()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entre_shock():\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    mr = merge_rank('./temp/lfs_a09_pqkb_ranked_V2_entrpreneurial_20200423.csv')\n",
    "    if not 'LFS_sector' in mr.columns:\n",
    "        mr = mr.rename(columns={'LFS_sector_x': 'LFS_sector'})\n",
    "        # get subset: a09_pqkb\n",
    "    mr_subset = mr[['hhid_lfs','cc101_lno','LFS_sector','a09_pqkb','c19_pclass','demand_scale', 'w_home','E_sector']]\n",
    "    mr_subset\n",
    "    indexNames = mr_subset[mr_subset['a09_pqkb'] == 'nan' ].index\n",
    "    # Delete these row indexes from dataFrame\n",
    "    mr_subset.drop(indexNames , inplace=True)\n",
    "    mr_subset = mr_subset.reset_index(drop=True)\n",
    " # get subset: c19_pclass\n",
    "\n",
    "    indexNames2 = mr_subset[mr_subset['c19_pclass'] == 'nan' ].index\n",
    "\n",
    "        # Delete these row indexes from dataFrame\n",
    "    mr_subset.drop(indexNames2 , inplace=True)\n",
    "    mr_subset = mr_subset.reset_index(drop=True)\n",
    "    mr_subset\n",
    "\n",
    "    # make new column of combined string a09 && c19:\n",
    "    mr_subset['a09c19'] = mr_subset['a09_pqkb'] +'-'+mr_subset['c19_pclass']\n",
    "\n",
    "        # enforce string:\n",
    "    mr_subset['a09_pqkb'] = [str(q).strip() for q in mr_subset['a09_pqkb']] # enforce type = string\n",
    "    mr_subset['LFS_sector'] = [str(q).strip() for q in mr_subset['LFS_sector']] # enforce type = string\n",
    "    mr_subset['c19_pclass'] = [str(q).strip() for q in mr_subset['c19_pclass']] # enforce type = string\n",
    "    mr_subset['a09c19'] = [str(q).strip() for q in mr_subset['a09c19']] # enforce type = string\n",
    "    if 'E_sector' in mr_subset.columns:\n",
    "        mr_subset['E_sector'] = [str(q).strip() for q in mr_subset['E_sector']] # enforce type = string\n",
    "\n",
    "    mr_subset\n",
    "\n",
    "\n",
    "    x = np.unique(mr_subset.c19_pclass)\n",
    "    x\n",
    "\n",
    "    ### create entrepreneurial table:\n",
    "    #rslt_df = dataframe[dataframe['Percentage'] > 80] \n",
    "    #subsetDataFrame = dfObj[dfObj['Product'].isin(['Mangos', 'Grapes']) ]\n",
    "\n",
    "\n",
    "    # or nonag_wage : private household, private establishment, govt corporation, with pay (family owned business)\n",
    "    df_nonag = mr_subset[~mr_subset['c19_pclass'].isin(['Self Employed', 'Employer','Without Pay (Family owned Business)'])]\n",
    "    df_nonag\n",
    "\n",
    "\n",
    "    # generate fraction by  ENTREPRENEURIAL sector\n",
    "    df_nonag['desc_count'] = df_nonag.groupby('a09_pqkb')['a09_pqkb'].transform('count')# count unique jobs and append to mr_subset\n",
    "    df_nonag['sector_count'] = df_nonag.groupby('E_sector')['E_sector'].transform('count') #count total unique sectors and append to mr_subset\n",
    "    df_nonag['sector_frac'] = df_nonag['desc_count'] / df_nonag['sector_count'] # get fraction of sector as weightin\n",
    "\n",
    "    #### for now we will leave non-ag here\n",
    "\n",
    "    # for entrepreneurial income: self employed, employer, withOUT pay (family owned business)\n",
    "    df_ent = mr_subset[mr_subset['c19_pclass'].isin(['Self Employed', 'Employer','Without Pay (Family owned Business)'])]\n",
    "    df_ent\n",
    "\n",
    "    # generate fraction by  ENTREPRENEURIAL sector\n",
    "    df_ent['desc_count'] = df_ent.groupby('a09_pqkb')['a09_pqkb'].transform('count')# count unique jobs and append to mr_subset\n",
    "    df_ent['sector_count'] = df_ent.groupby('E_sector')['E_sector'].transform('count') #count total unique sectors and append to mr_subset\n",
    "    df_ent['sector_frac'] = df_ent['desc_count'] / df_ent['sector_count'] # get fraction of sector as weightin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #     # generate fraction by sector\n",
    "    # mr_subset['desc_count'] = mr_subset.groupby('a09_pqkb')['a09_pqkb'].transform('count')# count unique jobs and append to mr_subset\n",
    "    # mr_subset['sector_count'] = mr_subset.groupby('LFS_sector')['LFS_sector'].transform('count') #count total unique sectors and append to mr_subset\n",
    "    # mr_subset['sector_frac'] = mr_subset['desc_count'] / mr_subset['sector_count'] # get fraction of sector as weighting\n",
    "\n",
    "\n",
    "\n",
    "    #####\n",
    "    # here, need to insert a new column that merges a09 and c19 -- done\n",
    "    # then, drop duplicates off of this column, so that we can minimize computation\n",
    "\n",
    "    # still need logic to build the logic for each job sector\n",
    "    ## may need to restructure this whole section of code\n",
    "\n",
    "    #####\n",
    "        # drop duplicates (now that overall weighting established)\n",
    "    df_ent = df_ent.drop_duplicates(subset='a09_pqkb')\n",
    "    df_ent = df_ent.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "        # generate probability and combine with relative weighting\n",
    "    df_ent['partial_prob'] = np.nan\n",
    "    df_ent['third_col'] = np.nan\n",
    "    df_ent['dummy'] = np.nan\n",
    "\n",
    "        # incorporate Kayenat tables into 'di' &&\n",
    "        # nested logic to incorporate 0-4 scale for social distancing measures\n",
    "        ## where scores of 0 & 1 result in complete job lost, due to unable to distance\n",
    "    i=0\n",
    "    while i < len(df_ent):\n",
    "\n",
    "        if df_ent.demand_scale[i] == 0:\n",
    "\n",
    "            # incorporate 0-4 scale logic:\n",
    "\n",
    "            if df_ent.w_home[i] == 0:\n",
    "                df_ent.partial_prob[i] = 0\n",
    "\n",
    "            elif df_ent.w_home[i] == 1:\n",
    "                df_ent.partial_prob[i] = 0\n",
    "\n",
    "            else:\n",
    "                df_ent.partial_prob[i] = df_ent.sector_frac[i] * (random.randint(0,50)/100)\n",
    "\n",
    "\n",
    "        elif df_ent.demand_scale[i] == 0.5: \n",
    "\n",
    "            # incorporate 0-4 scale logic:\n",
    "            if df_ent.w_home[i] == 0:\n",
    "                df_ent.partial_prob[i] = 0\n",
    "\n",
    "            elif df_ent.w_home[i] == 1:\n",
    "                df_ent.partial_prob[i] = 0\n",
    "\n",
    "            else: \n",
    "                df_ent.partial_prob[i] = df_ent.sector_frac[i] * (random.randint(50,100)/100)\n",
    "\n",
    "        elif df_ent.demand_scale[i] == 1.0:\n",
    "            df_ent.partial_prob[i] = df_ent.sector_frac[i]\n",
    "        else:\n",
    "            df_ent.dummy[i] = -99\n",
    "        i = i + 1\n",
    "\n",
    "    # incorporate 3rd column modifiers here:\n",
    "    # if (df_ent['c19_pclass'][i] == \"Gov't/Gov't Corporation\"):\n",
    "    #     df_ent.partial_prob[i] = 0  # essentially reverts the random uniform logic implemented above\n",
    "\n",
    "\n",
    "    i = i + 1\n",
    "\n",
    "        # remove nans in summing fields, and dummy storage\n",
    "    del df_ent['dummy']\n",
    "\n",
    "    df_ent['c19_pclass'] == \"Gov't/Gov't Corporation\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # get mean probability by sector:\n",
    "\n",
    "\n",
    "    #storage['fa'] = np.nan\n",
    "    storage = pd.DataFrame(columns=['fa', 'di'], index=[np.unique(df_ent.E_sector)])\n",
    "\n",
    "\n",
    "\n",
    "    for seclist in np.unique(df_ent.E_sector): # hard-coded to existing shock table\n",
    "\n",
    "        pp = df_ent[df_ent.E_sector == seclist]\n",
    "        p4 = 1 - sum(pp.partial_prob)\n",
    "\n",
    "        # build shock table:\n",
    "        storage['fa'][seclist] = storage['fa'][seclist] + p4\n",
    "        print(seclist)\n",
    "\n",
    "\n",
    "\n",
    "    # save to separate var for testing    \n",
    "    rand_weighted_shock = storage\n",
    "\n",
    "    rand_weighted_shock\n",
    "    #return(rand_weighted_shock)\n",
    "\n",
    "\n",
    "\n",
    "    storage = pd.DataFrame(columns = ['sector','fa'])\n",
    "    storage\n",
    "\n",
    "\n",
    "    storage = pd.DataFrame(columns=['fa', 'di'], index=[np.unique(df_ent.E_sector)])\n",
    "    storage\n",
    "\n",
    "    pp = df_ent[df_ent.E_sector == 'Construction']\n",
    "    pp\n",
    "\n",
    "    df_ent[df_ent.E_sector == 'Wholesale and Retail'].partial_prob.sum()\n",
    "\n",
    "    df_ent[df_ent.E_sector == 'Fishing'].partial_prob.sum()\n",
    "\n",
    "    np.unique(df_ent.E_sector) #### the spacing is fucking up the table!\n",
    "\n",
    "    storage = pd.DataFrame(columns=['fa', 'di'], index=[np.unique(df_ent.E_sector)])\n",
    "\n",
    "    for seclist in np.unique(df_ent.E_sector):\n",
    "        pillow = 1 - (df_ent[df_ent.E_sector == seclist].partial_prob.sum())\n",
    "    #     if pillow > 1:\n",
    "    #         pillow = 1\n",
    "        print(pillow)\n",
    "        #storage[seclist]['fa'] = df_ent[df_ent.E_sector == seclist].partial_prob.sum()\n",
    "        #df.loc[0:15,'A'] = 16\n",
    "        storage.loc[seclist,'fa'] = pillow\n",
    "\n",
    "\n",
    "    storage\n",
    "\n",
    "    #     tstamp = (datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "    #     storage.to_csv('./temp/entrep_table_problem_' +tstamp+'.csv')\n",
    "    \n",
    "    return(storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shock_100_entre():  # initialize shock sector storage dataframe\n",
    "   \n",
    "    '''\n",
    "   current hard coding for sensitivity analysis, 20200413: requires cleaning for further implementation\n",
    "   - addition of modularity\n",
    "   - \n",
    "   - current functionality:\n",
    "       - outputs csv to location: './temp/sect_iter_100.csv\n",
    "       - containing data frame with 101 simulations of <rand_weighted_shock_distance():\n",
    "    - runtime: ~10minutes\n",
    "   '''\n",
    "\n",
    "    stor = entre_shock()\n",
    "    del stor['di']\n",
    "\n",
    "    # set number of iterations\n",
    "    p = 0\n",
    "    n_iter = 99\n",
    "\n",
    "    # model and store stochastic sector response\n",
    "    while p < n_iter:\n",
    "        new_val = entre_shock()\n",
    "        del new_val['di']\n",
    "        new_val = new_val.rename(columns={'fa': ('iter'+str(p))})\n",
    "\n",
    "        # pd.merge(labor,rank, on=merge_col, how='left')\n",
    "        stor = pd.merge(stor,new_val,on='LFS_sector', how='left')\n",
    "        p = p+ 1\n",
    "        print(p)\n",
    "    tstamp = (datetime.now().strftime(\"%Y%m%d_%H%M\"))\n",
    "    stor.to_csv('./temp/entre_shock_mc__'+tstamp+ '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = generate_shock_100_entre()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
